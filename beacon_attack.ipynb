{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import timeit\n",
    "import itertools\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join, isfile\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import time\n",
    "from scipy.special import gamma\n",
    "\n",
    "from chronometer import ch # Easy chronometer that I implemented. First call: start, Second call: stop\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, formatter={'float': lambda x: \"{0:0.4f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(data_path):\n",
    "    beacon = pd.read_csv(join(data_path, \"Beacon_164.txt\"), index_col=0, delim_whitespace=True) # Needs to be sorted wrt rs100...\n",
    "    maf = pd.read_csv(join(data_path, \"MAF.txt\"), index_col=0, delim_whitespace=True)   # Alelle frequency of only chromosome 1 \n",
    "    af = pd.read_csv(join(data_path, \"AF_CEU_all_chromosomes.txt\"))   ## Alelle frequency of all chromosomes\n",
    "    ld = pd.read_csv(join(data_path, \"ld_new20_CEU_07_sortedM.txt\"), sep = \",\" , index_col=0)\n",
    "    \n",
    "    af = af.sort_values(by=[\"markerId\"]) # Needs to be sorted\n",
    "    \n",
    "    chromosome_index = 1 # Put in a JSON file\n",
    "    indices = (maf[\"chromosome\"] == \"chr\"+str(chromosome_index))\n",
    "    \n",
    "    maf = maf.loc[indices]  \n",
    "    beacon = beacon.loc[indices.to_numpy()]\n",
    "\n",
    "    return (beacon, af, ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a graph using columns of the LD file. The graph is directed and is used in QI Attack.\n",
    "# Since the column names are not proper, it is a good practice to change their names before using it.\n",
    "# Thereby, we will access the weights with \"weight\" keyword.\n",
    "# Otherwise we would have to access with \"Var11\" and \"Var12\" which is not intuitive.\n",
    "def construct_graph(ld):\n",
    "    ld.rename(columns={\"Var11\":\"weight\"}, inplace=True)\n",
    "    G1 = nx.from_pandas_edgelist(ld, \"Var4\", \"Var5\", [\"weight\"], create_using=nx.DiGraph())\n",
    "    ld.rename(columns={\"weight\":\"Var11\", \"Var12\":\"weight\"}, inplace=True)\n",
    "    G2 = nx.from_pandas_edgelist(ld, \"Var5\", \"Var4\", [\"weight\"], create_using=nx.DiGraph())\n",
    "    return nx.compose(G1, G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_biallelic(curAF, markers, snps):\n",
    "    ind = ~((snps == \"AA\") | (snps == \"CC\") | (snps == \"GG\") | (snps == \"NN\") | (snps == \"TT\"))\n",
    "\n",
    "    lower = curAF[\"referenceAlleleFrequency\"] < 0.5\n",
    "    snps[lower] = curAF[\"referenceAllele\"].loc[lower]\n",
    "    upper = ~lower\n",
    "    snps[upper] = curAF[\"otherAllele\"].loc[upper]\n",
    "\n",
    "    snps = snps[ind]\n",
    "    markers = markers[ind]\n",
    "    curAF = curAF[ind]\n",
    "    \n",
    "    return curAF, markers, snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function simulates the query system in beacon networks. It returns 0 or 1, not exist or exist, respectively.\n",
    "def query_beacon(marker, allele, beacon):\n",
    "    try:\n",
    "        vector = beacon.loc[marker].unique() # unique alleles \n",
    "        for i in range(len(vector)):\n",
    "            if vector[i][0] == allele or vector[i][1] == allele:\n",
    "                return 1\n",
    "        return 0\n",
    "    except KeyError:\n",
    "        print(\"No such marker\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power(power):\n",
    "    plt.box(False)\n",
    "    plt.xlabel(\"Number of Queries\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.xlim(0, len(power)-1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.plot(power)\n",
    "\n",
    "# Calculate power array using case delta and control delta results.\n",
    "def calculate_power(cs, cn, test_size, query_count, t_alpha, do_power_plot=True):\n",
    "    # Move all zeros to the end. ??\n",
    "    for i in range(test_size):\n",
    "        cs[i,:] = np.concatenate((cs[i,cs[i,:]!=0], cs[i,cs[i,:]==0]))\n",
    "        cn[i,:] = np.concatenate((cn[i,cn[i,:]!=0], cn[i,cn[i,:]==0]))\n",
    "\n",
    "    power = np.ones(query_count + 1)\n",
    "    power[0] = 0\n",
    "\n",
    "    for q in range(query_count):\n",
    "        if len(cn[cn[:,q] != 0, q]) == 0: # We are checking because otherwise percentile function gives error.\n",
    "            power[q+1] = 0\n",
    "            continue\n",
    "        # Count the elements above the threshold.\n",
    "        power[q+1] = np.sum(cs[cs[:,q] != 0, q] < np.percentile(cn[cn[:,q] != 0, q], t_alpha)) / np.sum(cs[:,q] != 0)\n",
    "    \n",
    "    if do_power_plot:\n",
    "        plot_power(power)\n",
    "    \n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta(error, af_term, NN, br):\n",
    "    log1 = np.log(np.square(1-af_term) / error)\n",
    "    log2 = np.log((error / np.square(1-af_term)) * ((1-np.power(1-af_term, 2*NN)) / (1-error*np.power(1-af_term, 2*NN-2))))\n",
    "    return np.sum(log1 + log2*br) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph function which calculates the sum of edges from the source \"source\" to possibly many targets \"targets\".\n",
    "# Notes:\n",
    "# To calculate, I use for loop which may be slow. However since the length of targets is not very high,\n",
    "# it did not cause a problem. But it is better to improve it if possible.\n",
    "def sum_of_edge_weights(graph, source, targets):\n",
    "    sum_of_weights = 0\n",
    "    for t in targets: # Can it be improved ??\n",
    "        sum_of_weights += graph[source][t][\"weight\"]\n",
    "    return sum_of_weights\n",
    "    \n",
    "# After calculating sum, it is divided by the length of targets.\n",
    "def mean_of_edge_weights(graph, source, targets):\n",
    "    return 0 if len(targets) == 0 else sum_of_edge_weights(graph, source, targets) / len(targets) \n",
    "\n",
    "# It finds all the edges from the \"source\" and returns target nodes which also exist in pass_nodes.\n",
    "def filter_edges(graph, source, pass_nodes):\n",
    "    edges = np.array(list(graph[source]))\n",
    "    return edges[np.in1d(edges, pass_nodes, assume_unique=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_SB(DN, DN_1, error, br, n):\n",
    "    log1 = np.log(DN / (DN_1 * error))\n",
    "    log2 = np.log((error * DN_1 * (1 - DN)) / (DN*(1-error*DN_1)))\n",
    "    return n*log1 + log2*np.sum(br)\n",
    "\n",
    "def get_DN(a, b, N):\n",
    "    return gamma(a + b) / (gamma(b) * (2*N + a + b)**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(beacon, h_vals, error, test_group, query_count, testAF, LD_Graph, attack_type=\"OPTIMAL\"):\n",
    "    \n",
    "    NN, test_size = beacon.shape[1], test_group.shape[1]    \n",
    "\n",
    "    Delta = np.zeros((len(h_vals), test_size, query_count))  \n",
    "\n",
    "    alreadyAsked = np.zeros((len(h_vals), test_size, query_count), dtype=bool)\n",
    "\n",
    "    for person in range(test_size): # test_size\n",
    "        print(\"[{}] - Person ID: {}\".format(person, test_group.columns[person]))\n",
    "        curAF, temp_markers, temp_snps = filter_biallelic(testAF, test_group.index.to_numpy(), np.array(test_group.iloc[:, person].values))\n",
    "        \n",
    "        for h in h_vals:\n",
    "            markers = np.array(temp_markers) # copy\n",
    "            snps = np.array(temp_snps) # copy\n",
    "            beacon_response = np.zeros((query_count), dtype=int)\n",
    "\n",
    "            min_af = np.minimum(curAF[\"referenceAlleleFrequency\"].values, curAF[\"otherAlleleFrequency\"].values)            \n",
    "\n",
    "            sorted_ind = np.lexsort((markers, min_af))\n",
    "            \n",
    "            min_af = min_af[sorted_ind] # Sorting based on minimum AF\n",
    "            snps = snps[sorted_ind] # Sorting based on minimum AF\n",
    "            markers = markers[sorted_ind] # Sorting based on minimum AF\n",
    "            \n",
    "            min_af[min_af == 0] = 1e-9\n",
    "            mask = np.where(min_af < (h / 100)) \n",
    "            markers = np.delete(markers, mask)\n",
    "            snps = np.delete(snps, mask)\n",
    "            min_af = np.delete(min_af, mask)\n",
    "            \n",
    "            innersum = 0\n",
    "            \n",
    "            markers = markers[:query_count]\n",
    "            min_af = min_af[:query_count]\n",
    "\n",
    "            for q in range(query_count):\n",
    "                \n",
    "                beacon_response[q] = query_beacon(markers[q], snps[q], beacon)\n",
    "    \n",
    "                if attack_type == \"OPTIMAL\":\n",
    "                    dlt = calculate_delta(error, min_af[:q+1], NN, beacon_response[:q+1])\n",
    "                    Delta[h_vals[h], person, q] = dlt\n",
    "\n",
    "                elif attack_type == \"QI\":\n",
    "\n",
    "                    if alreadyAsked[h_vals[h], person, q]:\n",
    "                        continue\n",
    "                    \n",
    "                    if markers[q] in LD_Graph :\n",
    "\n",
    "                        nodes = np.array(sorted(list(LD_Graph[markers[q]])))\n",
    "                        indices = np.in1d(nodes, markers, assume_unique=True)\n",
    "                        nodes = nodes[indices]\n",
    "                        \n",
    "                        indices = np.in1d(markers, nodes, assume_unique=True) # Intersection\n",
    "                        MAFinner = min_af[indices]\n",
    "                        \n",
    "                        nodes = np.concatenate((markers[indices], [markers[q]])) # Append itself\n",
    "                     \n",
    "                        # Why we are appending if we don't use it.\n",
    "                        \n",
    "                        if len(MAFinner) > 0:\n",
    "                            alreadyAsked[h_vals[h], person, indices[:query_count]] = True \n",
    "                            \n",
    "                            if len(nodes) > 0: # used to be 1. But why? Definitely greater than zero\n",
    "                                w_sum = np.zeros((len(nodes), 3))\n",
    "                                \n",
    "                                for j in range(len(nodes)):\n",
    "                                    edges = filter_edges(LD_Graph, nodes[j], nodes)\n",
    "\n",
    "                                    w_sum[j, 0] = mean_of_edge_weights(LD_Graph, nodes[j], edges)\n",
    "                                    w_sum[j, 1] = len(edges)\n",
    "                                    w_sum[j, 2] = j\n",
    "                                    \n",
    "                                w_sum = w_sum[np.argsort(w_sum[:, 1]),:] # Sort based on the second col.\n",
    "                                w_sum = w_sum[:round(len(nodes) * 0.3),:]    # 0.3 should be an argument?\n",
    "                                query_m = nodes[int(w_sum[np.argmax(w_sum[:,0]), 2])]\n",
    "\n",
    "                                nodes = np.array(list(LD_Graph[query_m]))\n",
    "                                indices = np.in1d(markers, nodes, assume_unique=True)\n",
    "                                MAFinner = min_af[indices]\n",
    "                                \n",
    "                                edges = filter_edges(LD_Graph, query_m, markers) # nodes\n",
    "                                mean = mean_of_edge_weights(LD_Graph, query_m, edges)\n",
    "                                allele = snps[np.where(markers==query_m)[0][0]] # Find the index of query_m, then access to snps.\n",
    "                                \n",
    "                                innersum += calculate_delta(error, MAFinner, NN, query_beacon(query_m, allele, beacon))*mean\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    mask = ~alreadyAsked[h_vals[h], person, :1+q]\n",
    "                    Delta[h_vals[h], person, q] = innersum + calculate_delta(error, min_af[:1+q][mask], NN, beacon_response[:1+q][mask])\n",
    "\n",
    "    return Delta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_SB(beacon, error, test_group, query_count, testAF, a_prime, b_prime):\n",
    "    \n",
    "    NN, test_size = beacon.shape[1], test_group.shape[1] \n",
    "        \n",
    "    DN = get_DN(a_prime + 1, b_prime + 1, NN)\n",
    "    DN_1 = get_DN(a_prime + 1, b_prime + 1, NN-1)\n",
    "    \n",
    "    Delta = np.zeros((1, test_size, query_count))  # Only for h = 0\n",
    "    \n",
    "    for person in range(test_size): # test_size\n",
    "#         print(\"[{}] - Person ID: {}\".format(person, test_group.columns[person]))\n",
    "        curAF, markers, snps = filter_biallelic(testAF, test_group.index.to_numpy(), np.array(test_group.iloc[:, person].values))\n",
    "        \n",
    "        beacon_response = np.zeros((query_count), dtype=int)\n",
    "        ind = np.random.permutation(len(markers))  # Select random markers in each iteration.\n",
    "        \n",
    "        for q in range(query_count):\n",
    "            beacon_response[q] = query_beacon(markers[ind[q]], snps[ind[q]], beacon)       \n",
    "            Delta[0, person, q] = calculate_delta_SB(DN, DN_1, error, beacon_response, q+1) \n",
    "    \n",
    "    return Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix_count(prefix, remaining_cor):\n",
    "    total = np.sum(remaining_cor == np.array([prefix]).T, axis=0)\n",
    "    return len(total[total == remaining_cor.shape[0]])\n",
    "    \n",
    "def calculate_prob(prefix, remaining_cor):\n",
    "    # correlations => 5x84\n",
    "    counts = [0, 0, 0]\n",
    "    for i in range(len(counts)):\n",
    "        counts[i] = get_prefix_count(prefix+[i], remaining_cor)\n",
    "\n",
    "    total = sum(counts)\n",
    "    if total == 0:        # We want 2 zero elements. 0 0 0 is not valid. 10 0 0 is valid. 8 2 0 is not valid.\n",
    "        return False\n",
    "    if total in counts:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def enumerate_alleles(remaining_GI, testAF, individual=False):\n",
    "    \n",
    "    m = remaining_GI.shape[0]\n",
    "    table = remaining_GI.copy(deep=True)\n",
    "    \n",
    "    if individual:\n",
    "        table = table.to_frame() # Do this when table is a series\n",
    "    \n",
    "    for i in range(m):\n",
    "        marker = table.index[i]\n",
    "        curAF = testAF[testAF[\"markerId\"] == marker]\n",
    "        if len(curAF) == 0:\n",
    "            return 0 # Is it the best way?\n",
    "        \n",
    "        allele = curAF.iloc[0][\"referenceAllele\"]\n",
    "        if curAF.iloc[0][\"referenceAlleleFrequency\"] > 0.5:\n",
    "            allele = curAF.iloc[0][\"otherAllele\"]\n",
    "        \n",
    "        regexKeys = [\"NN\", \"TT\", \"GG\", \"AA\", \"CC\", \"..\"]\n",
    "        regexVals = [-1, 0, 0, 0, 0, 1]\n",
    "        \n",
    "        regexVals[regexKeys.index(allele*2)] = 2\n",
    "        table.iloc[i].replace(regexKeys, regexVals, inplace=True, regex=True)\n",
    "    \n",
    "    table = np.array(table.to_numpy())\n",
    "    for i in range(m):\n",
    "        table = table[:, table[i, :] != -1]\n",
    "    \n",
    "    return table\n",
    "\n",
    "            \n",
    "def attack_GI(beacon, h_vals, error, test_group, remaining_GI, query_count, testAF, order):\n",
    "    \n",
    "    NN, test_size = beacon.shape[1], test_group.shape[1]    \n",
    "    Delta = np.zeros((len(h_vals), test_size, query_count)) # Init delta\n",
    "    \n",
    "    for person in range(test_size): # test_size\n",
    "        print(\"[{}] - Person ID: {}\".format(person, test_group.columns[person]))\n",
    "        curAF, markers, snps = filter_biallelic(testAF, test_group.index.to_numpy(), np.array(test_group.iloc[:, person].values))\n",
    "        \n",
    "        CHR = test_group.iloc[:,person]\n",
    "        \n",
    "        CHR_ind = dict(zip(CHR.index, range(len(CHR.index))))\n",
    "        \n",
    "        min_af = np.minimum(curAF[\"referenceAlleleFrequency\"].values, curAF[\"otherAlleleFrequency\"].values)            \n",
    "        \n",
    "        sorted_ind = np.lexsort((markers, min_af))\n",
    "        \n",
    "        min_af = min_af[sorted_ind] # Sorting based on minimum AF\n",
    "        snps = snps[sorted_ind] # Sorting based on minimum AF\n",
    "        markers = markers[sorted_ind] # Sorting based on minimum AF\n",
    "        \n",
    "        min_af[min_af == 0] = 1e-9 # We don't do the same thing for original AF. May be a problem.\n",
    "        \n",
    "        for h in h_vals:\n",
    "            \n",
    "            beacon_response = np.zeros((query_count), dtype=int)\n",
    "            maf = np.zeros((query_count))\n",
    "            \n",
    "            to_infer_idx = np.where(min_af < (h / 100))[0] # since it is a tuple. However, it does not affect the delete func. \n",
    "            \n",
    "            q = 0\n",
    "            for i in range(len(to_infer_idx)):\n",
    "                if q == query_count:\n",
    "                    break \n",
    "                    \n",
    "                curMarker = markers[to_infer_idx[i]]\n",
    "\n",
    "#                 CHR_idx = np.where(CHR.index == curMarker)[0][0] # => THIS STATEMENT SLOWS IT DOWN \n",
    "                CHR_idx = CHR_ind[curMarker]                       # => THIS IS A LOT FASTER (HASHING INDICES)\n",
    "                \n",
    "                if CHR_idx < order: \n",
    "                    continue\n",
    "                \n",
    "                cont = False\n",
    "                for o in range(CHR_idx - order, CHR_idx + 1):\n",
    "                    if CHR[o] == \"NN\":\n",
    "                        cont = True\n",
    "                        break\n",
    "                if cont:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                 # === SLOW PART BEGINS ===================================================================\n",
    "                    \n",
    "                correlations = enumerate_alleles(remaining_GI.iloc[CHR_idx - order:CHR_idx + 1], testAF, False)\n",
    "                \n",
    "#                 I am not sure whether we should check 4 elements or 5 elements ??\n",
    "                prefix = enumerate_alleles(CHR[CHR_idx-order:CHR_idx], testAF, True)\n",
    "                prefix = list(prefix.flatten())\n",
    "                \n",
    "                if not calculate_prob(prefix, correlations):\n",
    "                    continue\n",
    "                \n",
    "                # === SLOW PART ENDS ======================================================================\n",
    "                \n",
    "                maf[q] = min_af[to_infer_idx[i]]\n",
    "                beacon_response[q] = query_beacon(markers[to_infer_idx[i]],  snps[to_infer_idx[i]], beacon)\n",
    "                dlt = calculate_delta(error, maf[:q+1], NN, beacon_response[:q+1])\n",
    "                Delta[h_vals[h], person, q] = dlt \n",
    "                \n",
    "                q += 1\n",
    "        \n",
    "    return Delta     \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the application begins after cold start.\n",
    "def start_test(attack_type, test_beacon, t_alpha, error, query_count, NN, h_vals, test_groups, remaining_GI, af, LD_Graph, a_prime, b_prime, order=4, do_power_plot=True):\n",
    "    \n",
    "    delta_values = []\n",
    "    delta = None\n",
    "    \n",
    "    for test_group in test_groups:\n",
    "        ch(attack_type)\n",
    "        if attack_type == \"GI\":\n",
    "            delta = attack_GI(test_beacon, h_vals, error, test_group[1], remaining_GI, query_count, af, order)\n",
    "        \n",
    "        elif attack_type == \"SB\":\n",
    "            delta = attack_SB(test_beacon, error, test_group[1], query_count, af, a_prime, b_prime)\n",
    "            \n",
    "        elif attack_type in (\"QI\", \"OPTIMAL\"):\n",
    "            delta = attack(test_beacon, h_vals, error, test_group[1], query_count, af, LD_Graph, attack_type)\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "        ch(attack_type)\n",
    "        delta_values.append(delta)\n",
    "    \n",
    "    \n",
    "    # ====================== PLOTTING\n",
    "    for h in h_vals:\n",
    "        power = calculate_power(delta_values[0][h_vals[h],:,:], delta_values[1][h_vals[h],:,:], test_size, query_count, t_alpha, do_power_plot=True)                \n",
    "    plt.show()\n",
    "    # ===============================\n",
    "    \n",
    "    # Note that calculate_power function may change the order of delta. If you want to return the original delta values,\n",
    "    # return or copy above the plotting part.\n",
    "    return delta_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold start\n",
    "ch(reset=True)\n",
    "data_path = \"./data/\" \n",
    "\n",
    "ch(\"Loading\")\n",
    "beacon, init_af, ld = load_files(data_path)\n",
    "af = init_af[init_af[\"markerId\"].isin(beacon.index)] # Get rid of unnecessary lines in the attack code.\n",
    "ch(\"Loading\")\n",
    "\n",
    "ch(\"Constructing Graph\")\n",
    "LD_Graph = construct_graph(ld)\n",
    "ch(\"Constructing Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly construct the beacon and the test groups \n",
    "\n",
    "idx = np.random.permutation(len(beacon.keys()))\n",
    "\n",
    "# Case (20)\n",
    "case = beacon.iloc[:,idx[0:20]]\n",
    "\n",
    "# Beacon (60)\n",
    "test_beacon = beacon.iloc[:, idx[0:60]]\n",
    "\n",
    "# Control (20)\n",
    "control = beacon.iloc[:, idx[60:80]]\n",
    "\n",
    "# Remaining for GI (84)\n",
    "remaining_GI = beacon.iloc[:, idx[80:]]\n",
    "\n",
    "test_groups = [(\"case\", case), (\"control\", control)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "h_vals = [0, 3, 5]\n",
    "NN = 60\n",
    "test_size = 20\n",
    "query_count = 2000\n",
    "error = 0.001\n",
    "t_alpha = 5\n",
    "order = 4\n",
    "h_vals = dict(zip(h_vals, range(len(h_vals))))\n",
    "\n",
    "a_prime = 0.0735 # 0.1300 # \n",
    "b_prime = 1.0096 # 1.1300 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ch(reset=True)\n",
    "deltas = start_test(\"SB\", test_beacon, t_alpha, error, query_count, NN, h_vals, test_groups, remaining_GI, af, LD_Graph, a_prime, b_prime, order=order, do_power_plot=True)\n",
    "ch(reset=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
